{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2569c6",
   "metadata": {},
   "source": [
    "# Vectorstore retrieval\n",
    "\n",
    "### Similarity Search\n",
    "\n",
    "Retrieval is the centerpiece of our retrieval augmented generation (RAG) flow. \n",
    "\n",
    "Given a question, we want to retrieve relevant splits.\n",
    "\n",
    "Last time, we saw that we can [search for](https://www.pinecone.io/learn/what-is-similarity-search/) semantically relevant docs from our vectordb.\n",
    "\n",
    "Let's recap that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf4794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "vectordb = FAISS.load_local(\"docs/cs229_lecture1_faiss_index\",embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6143dd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29e8c9",
   "metadata": {},
   "source": [
    "### Maximum marginal relevance\n",
    "\n",
    "Now, let's go a bit beyond this.\n",
    "\n",
    "What happens in we want to ensure diversity in the resulting chunks?\n",
    " \n",
    "Many vectorstores offer additional search methods to support this.\n",
    " \n",
    "For example, using `maximum marginal relevance` [strives to achieve](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) both relevance to the query *and diversity* among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222234c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "len(docs_mmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b909bc",
   "metadata": {},
   "source": [
    "### Working with metadata\n",
    "\n",
    "In addition, many vectorstores support operations on `metadata`.\n",
    "\n",
    "This provides context for each embedded chunk.\n",
    "\n",
    "But [we have an interesting challenge](https://twitter.com/hwchase17/status/1651617956881924096?s=20): \n",
    "\n",
    "Vector search is good for capturing semantically similar text. \n",
    "\n",
    "But, often queries specify desired attributes like time, authorship, or other \"metadata\" fields.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    " \n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in as well\n",
    "\n",
    "Most vector databases support metadata filters, [so this doesn't require any new databases or indexes](https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/chroma_self_query).\n",
    "\n",
    "Let's try it out with Chroma!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lark\n",
    "! pip install chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cacaca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19367d52",
   "metadata": {},
   "source": [
    "Define our data and create the vectorDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309f0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"rating\": 9.9,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"science fiction\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426eb17",
   "metadata": {},
   "source": [
    "We need to provide some (minimal) information on the fields in the metadata available to filter on.\n",
    "\n",
    "Now a combination of a query and metadata filter\n",
    "\n",
    "User Query: \"Has Greta Gerwig directed any movies about women\"\n",
    "\n",
    "```\n",
    "Extracted Query: women\n",
    "Metadata Filter: director = Greta Gerwig\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d06084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'year': 2019, 'director': 'Greta Gerwig', 'rating': 8.3})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectorstore, document_content_description, metadata_field_info, verbose=True\n",
    ")\n",
    "\n",
    "retriever.get_relevant_documents(\"Has Greta Gerwig directed any movies about women\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6f410",
   "metadata": {},
   "source": [
    "### Combining ideas \n",
    "\n",
    "Let's apply self-query retriver to a Notion database.\n",
    "\n",
    "And split the Notion database using `MarkdownHeaderTextSplitter`, which will populate the headers.\n",
    "\n",
    "We can apply it to [this](https://rlancemartin.notion.site/Auto-Evaluation-of-Metadata-Filtering-18502448c85240828f33716740f9574b?pvs=4) Notion doc as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1851a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Notion database as a markdownfile file\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"docs/Notion_DB_Metadata\")\n",
    "docs = loader.load()\n",
    "md_file=docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10ccdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'We previously introduced [auto-evaluator](https://blog.langchain.dev/auto-evaluator-opportunities/), an open-source tool for grading LLM question-answer chains. Here, we extend auto-evaluator with a [lightweight Streamlit app](https://github.com/langchain-ai/auto-evaluator/tree/main/streamlit) that can connect to any existing Pinecone index. We add the ability to test metadata filtering using `SelfQueryRetriever` as well as some other approaches that weâve found to be useful, as discussed below.  \\n[ret_trim.mov](Auto-Evaluation%20of%20Metadata%20Filtering%2018502448c85240828f33716740f9574b/ret_trim.mov)',\n",
       " 'metadata': {'Section': 'Evaluation'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we have the doc grouped by the headers\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"###\", \"Section\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(md_file)\n",
    "md_header_splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cffcefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk_size = 100\n",
    "chunk_overlap = 0\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    " \n",
    "# Create splits within each header group and combine them\n",
    "all_splits=[]\n",
    "all_metadatas=[]\n",
    "for header_group in md_header_splits:\n",
    "    _splits = text_splitter.split_text(header_group['content'])\n",
    "    _metadatas = [header_group['metadata'] for _ in _splits]\n",
    "    all_splits += _splits\n",
    "    all_metadatas += _metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b650cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vectorstore and keep the metadata\n",
    "vectorstore = Chroma.from_texts(texts=all_splits,metadatas=all_metadatas,embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b23ff85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever \n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "# Define our metadata\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Section\",\n",
    "        description=\"Part of the document that the text comes from\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Major sections of the document\"\n",
    "\n",
    "# Define self query retriver\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(llm, vectorstore, document_content_description, metadata_field_info, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0fd303",
   "metadata": {},
   "source": [
    "We can see that we query *only for texts* in the Introduction of the document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceeb2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='Introduction' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='Section', value='Introduction') limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='.png)', metadata={'Section': 'Introduction'}),\n",
       " Document(page_content='![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%2018502448c85240828f33716740f9574b/Untitled', metadata={'Section': 'Introduction'}),\n",
       " Document(page_content='Retriever-Less option (at bottom in the below diagram), highlighting the Anthropic 100k context', metadata={'Section': 'Introduction'}),\n",
       " Document(page_content='into an answer. There many ways to approach this. For example, we recently', metadata={'Section': 'Introduction'})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "retriever.get_relevant_documents(\"Summarize the Introduction section of the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41872f09",
   "metadata": {},
   "source": [
    "### < Placeholder: Contextual Compression, Time weighting >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b63e1",
   "metadata": {},
   "source": [
    "# Other types of retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d868d65",
   "metadata": {},
   "source": [
    "It's worth noting that vectordb as not the only kind of tool to retrieve documents. \n",
    "\n",
    "The `LangChain` [retriever abstraction](https://blog.langchain.dev/retrieval/) includes other ways to retrieve documents, such as [TF-IDF](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558) or [SVMs](https://twitter.com/hwchase17/status/1647328542529843200?s=20). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83d2e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)\n",
    "\n",
    "# Retrieve\n",
    "svm_retriever = SVMRetriever.from_texts(splits,embeddings)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1cc35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are major topics for this class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76ce1c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"let me just check what questions you have righ t now. So if there are no questions, I'll just \\nclose with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.  \\nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.   [End of Audio]  \\nDuration: 69 minutes\", metadata={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
