{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e7f19a",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "Recall last time we discussed document loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dc327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('docs/cs229_lecture1_whisper_transcript.txt', encoding='utf8')\n",
    "text = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f720db",
   "metadata": {},
   "source": [
    "# Text splitting - Charecters and Tokens\n",
    "\n",
    "LLMs have limited context windows, meaning a limited prompt length can be passed into the model.\n",
    "\n",
    "At the larger end, Anthropic's Claude model has a 100k token context windw.\n",
    " \n",
    "But, ChatGPT (gpt-3.5) only has 4096 tokens.\n",
    "\n",
    "Tokens are often [~4 charecters](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them).\n",
    "\n",
    "So, let's have a look at the PDF we loaded to check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc96cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "print(len(joined_page_text)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c875da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text[0].page_content)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 150\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size,chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc89f8c",
   "metadata": {},
   "source": [
    "Note that we define two important parameters:\n",
    "    \n",
    "`chunk_size = 1500`\n",
    "\n",
    "`chunk_overlap = 150`\n",
    "\n",
    "These are the length of each chunk (in charecters) and the overlap between them.\n",
    "\n",
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(text[0].page_content)\n",
    "print(len(splits))\n",
    "print(len(splits[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de437b",
   "metadata": {},
   "source": [
    "Good, so we can see that it set the expected chunk size. \n",
    "\n",
    "Later, we will discuss how to pick the best chunk_size and overlap.\n",
    "\n",
    "For now, the selections show above 1500, 150 are reasonable default values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb0cd9",
   "metadata": {},
   "source": [
    "We can also split on [token count explicity](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token), if we want.\n",
    "\n",
    "This can be useful because LLMs often have context windows [designated in tokens](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa29d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(text[0].page_content)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d9bfa",
   "metadata": {},
   "source": [
    "# Text splitting - Preserving context\n",
    "\n",
    "Sometimes we want to preserve the local context of certain text groups.\n",
    "\n",
    "For example, a markdown file is organized by headers. \n",
    "\n",
    "We might want to only split text within certain header groups to keep them together.\n",
    "\n",
    "[These notes](https://www.pinecone.io/learn/chunking-strategies/) from Pinecone provide additional motivation:\n",
    "\n",
    "```\n",
    "When a full paragraph or document is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text. Larger input text sizes, on the other hand, may introduce noise or dilute the significance of individual sentences or phrases, making finding precise matches when querying the index more difficult.\n",
    "```\n",
    "\n",
    "In particular, if we mix chunks across header groups we may degregate the retrieval quality.\n",
    "\n",
    "We can use `MarkdownHeaderTextSplitter` to [preserve header metadata](https://github.com/hwchase17/langchain/blob/master/docs/extras/modules/data_connection/document_transformers/text_splitters/markdown_header_metadata.ipynb) in our chunks, as showm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b238b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60d3c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('docs/Notion_DB/Blendle.md')\n",
    "text = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fdab1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"This is a living document with everything we've learned working with people while running a startup. And, of course, we continue to learn. Therefore it's a document that will continue to change.  \\n**Everything related to working at Blendle and the people of Blendle, made public.**  \\nThese are the lessons from three years of working with the people of Blendle. It contains everything from [how our leaders lead](https://www.notion.so/ecfb7e647136468a9a0a32f1771a8f52?pvs=21) to [how we increase salaries](https://www.notion.so/Salary-Review-e11b6161c6d34f5c9568bb3e83ed96b6?pvs=21), from [how we hire](https://www.notion.so/Hiring-451bbcfe8d9b49438c0633326bb7af0a?pvs=21) and [fire](https://www.notion.so/Firing-5567687a2000496b8412e53cd58eed9d?pvs=21) to [how we think people should give each other feedback](https://www.notion.so/Our-Feedback-Process-eb64f1de796b4350aeab3bc068e3801f?pvs=21) â€” and much more.  \\nWe've made this document public because we want to learn from you. We're very much interested in your feedback (including weeding out typo's and Dunglish ;)). Email us at hr@blendle.com. If you're starting your own company or if you're curious as to how we do things at Blendle, we hope that our employee handbook inspires you.  \\nIf you want to work at Blendle you can check our [job ads here](https://blendle.homerun.co/). If you want to be kept in the loop about Blendle, you can sign up for [our behind the scenes newsletter](https://blendle.homerun.co/yes-keep-me-posted/tr/apply?token=8092d4128c306003d97dd3821bad06f2).\",\n",
       " 'metadata': {'Header 1': \"Blendle's Employee Handbook\"}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we have splits grouped by the specified headers! \n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(text[0].page_content)\n",
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b091fe0",
   "metadata": {},
   "source": [
    "Within each markdown group we can then apply any splitter we want e.g., `TokenTextSplitter` or `RecursiveCharacterTextSplitter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e89f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk_size = 100\n",
    "chunk_overlap = 0\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    " \n",
    "# Create splits within each header group and combine them!\n",
    "all_splits=[]\n",
    "all_metadatas=[]\n",
    "for header_group in md_header_splits:\n",
    "    _splits = text_splitter.split_text(header_group['content'])\n",
    "    _metadatas = [header_group['metadata'] for _ in _splits]\n",
    "    all_splits += _splits\n",
    "    all_metadatas += _metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f54159e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a living document with everything we've learned working with people while running a startup.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91061b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Header 1': \"Blendle's Employee Handbook\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadatas[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
